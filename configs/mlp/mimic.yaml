# Network
network_layers: [32]
activiation_fn: "relu"
dropout_rate: 0.5
l2_reg: 0.001
batch_size: 128
early_stop: True
patience: 5
n_samples_train: 10
n_samples_valid: 10
n_samples_test: 100

optimizer:
  class_name: Adam
  config:
    learning_rate: 0.005
    decay: 0.0001
